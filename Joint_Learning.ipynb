{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhnZqIb8E0mu",
    "outputId": "a0a0f3c7-172d-43a1-a4f8-dcc668e346fd"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "your_module = drive.CreateFile({'id':'174Ws0LnaEQdK7UmOKh-8GJwftO18Mfej'})\n",
    "your_module.GetContentFile('metric.py')\n",
    "your_module1 = drive.CreateFile({'id':'11ssSCDodnZ6_jXcHLZ8AqYHbg7g7K8Be'})\n",
    "your_module1.GetContentFile('threshold.py')\n",
    "your_module2 = drive.CreateFile({'id':'1JpuTOGEKMq9iZcO6Tp4ET9icd5EG0P0D'})\n",
    "your_module2.GetContentFile('cat_list.py')\n",
    "your_module2 = drive.CreateFile({'id':'1gsMrGL0BfVASQICf-eesMRelmCgMLTYs'})\n",
    "your_module2.GetContentFile('cat_list_show.py')\n",
    "from metric import *\n",
    "from threshold import *\n",
    "from cat_list import *\n",
    "from cat_list_show import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPYld0Nm_Zjn"
   },
   "outputs": [],
   "source": [
    "def jacard(y_test, predictions):\n",
    "    accuracy = 0.0\n",
    "\n",
    "    for i in range(y_test.shape[0]):\n",
    "        intersection = 0.0\n",
    "        union = 0.0\n",
    "        for j in range(y_test.shape[1]):\n",
    "            if int(y_test[i,j]) == 1 or int(predictions[i,j]) == 1:\n",
    "                union += 1\n",
    "            if int(y_test[i,j]) == 1 and int(predictions[i,j]) == 1:\n",
    "                intersection += 1\n",
    "            if int(y_test[i,j]) == 2 or int(predictions[i,j]) == 2:\n",
    "                union += 1\n",
    "            if int(y_test[i,j]) == 2 and int(predictions[i,j]) == 2:\n",
    "                intersection += 1\n",
    "            \n",
    "        if union != 0:\n",
    "            accuracy = accuracy + float(intersection/union)\n",
    "\n",
    "    accuracy = float(accuracy/y_test.shape[0])\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K2HoFkqWFMfj",
    "outputId": "c143d7fe-ebe3-43f7-a2ac-8e5235e05591"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-EQHORIeE0nS",
    "outputId": "901353b6-0436-4dac-8b0b-e4fef5de8bc2"
   },
   "outputs": [],
   "source": [
    "#تکرار پذیری مدل\n",
    "import os\n",
    "sd = 1024\n",
    "os.environ['PYTHONHASHSEED']=str(sd)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import random as rn\n",
    "random.seed(sd)\n",
    "np.random.seed(sd)\n",
    "rn.seed(sd)\n",
    "from keras import backend as K\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "tf.random.set_seed(sd)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "tf.compat.v1.keras.backend.set_floatx('float64')\n",
    "initializer = tf.random_normal_initializer(0., 0.02, seed=sd)\n",
    "#فراخوانی کتابخانه ها\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras.models import Model,Sequential\n",
    "from tensorflow.python.keras.layers import Input,Dense, GRU, Embedding, LSTM,Conv1D\n",
    "from tensorflow.keras.optimizers import Adam,Adagrad,Nadam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dropout,GlobalMaxPooling1D,Conv1D,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tensorboard = TensorBoard(log_dir='drive/My Drive/logs/{}'.format(time()))\n",
    "\n",
    "#فراخوانی داده های آموزشی و آزمایشی\n",
    "dataset = pd.read_excel('/content/drive/MyDrive/تحلیل احساسات سطح ویژگی/train.xlsx')\n",
    "test_file = pd.read_excel('/content/drive/MyDrive/تحلیل احساسات سطح ویژگی/test.xlsx')\n",
    "S_comments_labels = dataset[['بازیگر','بازیگری','داستان',\n",
    "                              'سبک','فیلم',\n",
    "                             'فیلم_نامه',\n",
    "                             'محتوا',\n",
    "                            'موضوع','کارگردان',\n",
    "                            ]]\n",
    "\n",
    "S_test_comments_labels = test_file[['بازیگر','بازیگری','داستان',\n",
    "                              'سبک','فیلم',\n",
    "                             'فیلم_نامه',\n",
    "                             'محتوا',\n",
    "                            'موضوع','کارگردان',\n",
    "                            ]]\n",
    "category_list=['بازیگر','بازیگری','داستان',\n",
    "                              'سبک','فیلم',\n",
    "                             'فیلم_نامه',\n",
    "                             'محتوا',\n",
    "                            'موضوع','کارگردان',\n",
    "                            ]\n",
    "stop_words =['به','شکل','ممکن','میشه','دلیل','رو','فقط','در','خانم','آقای','اگر','مریلا',\n",
    "            'این','هم','اسامی','زارعی','بهرام','رادان','حیایی','پارسا','پیروزفر','و','صابر','ابر','حال','های',\n",
    "            'ها','یه','ای','بابت','آبیار','از','تا','هفتاد','درصد','الناز','دو','اول','شاکردوست','سیمرغ','زن','تاریخ','شاکر','هاش','ریگی','اش','معادی','نرگس','دوم','ابیار','پیمان','اقای','بهداد','مهناز','حامد','عطاران','علی','نوید','ام','امیر','۱۰','محمدزاده','نفر','تیکت','سعید','لباس','خدا','هانیه'\n",
    "             ,'توسلی','سحر','۲۰','ثقفی','رضا','کریمی','روسی','۴','کامنت','عبدالمالک','ش','اکبر','هیس','مهرجویی','طباطبایی','سرخپوست','عبدی','شش','شیطان','پرستی'\n",
    "            ]\n",
    "#پیش پردازش\n",
    "import re\n",
    "def preprocess_text(sentence):\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'@\\S+', '', sentence)\n",
    "    sentence = re.sub(r'!\\S+', '', sentence)\n",
    "    sentence = re.sub(r'؟\\S+', '', sentence)\n",
    "    sentence = re.sub(r'[.]', ' ', sentence)\n",
    "    sentence = re.sub(r'[/]', '', sentence)\n",
    "    sentence = re.sub(r'[،]', ' ', sentence)\n",
    "    sentence = re.sub(r'[؛]', '', sentence)\n",
    "    sentence = sentence.split()\n",
    "    sentence =[word for word in sentence if word not in stop_words]\n",
    "    sentence = ' '.join(sentence)\n",
    "    return sentence\n",
    "def ReplacetwoMore(s):\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\", s)\n",
    "dataset['clean_text'] = dataset['Sentence'].apply(ReplacetwoMore)\n",
    "test_file['clean_text'] = test_file['Sentence'].apply(ReplacetwoMore)\n",
    "X = []\n",
    "sentences = list(dataset[\"clean_text\"])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))    \n",
    "X_t = []\n",
    "sentences2 = list(test_file[\"clean_text\"])\n",
    "for sen in sentences2:\n",
    "    X_t.append(preprocess_text(sen))\n",
    "np.random.seed(sd)\n",
    "\n",
    "y = S_comments_labels.values\n",
    "y_test1=S_test_comments_labels.values\n",
    "target =y\n",
    "data = X\n",
    "target1 = y_test1\n",
    "data1 = X_t\n",
    "x_train, x_test = data, data1\n",
    "y_train, y_test = target, target1\n",
    "np.random.seed(sd)\n",
    "print(x_train[136])\n",
    "print(y_train[136])\n",
    "y1_train = dataset[[\"بازیگر\"]].values\n",
    "y1_test =  test_file[[\"بازیگر\"]].values\n",
    "y2_train = dataset[[\"بازیگری\"]].values\n",
    "y2_test =  test_file[[\"بازیگری\"]].values\n",
    "y3_train = dataset[[\"داستان\"]].values\n",
    "y3_test =  test_file[[\"داستان\"]].values\n",
    "\n",
    "y4_train = dataset[[\"سبک\"]].values\n",
    "y4_test =  test_file[[\"سبک\"]].values\n",
    "y5_train = dataset[[\"فیلم\"]].values\n",
    "y5_test =  test_file[[\"فیلم\"]].values\n",
    "y6_train = dataset[[\"فیلم_نامه\"]].values\n",
    "y6_test =  test_file[[\"فیلم_نامه\"]].values\n",
    "\n",
    "\n",
    "y7_train = dataset[[\"محتوا\"]].values\n",
    "y7_test =  test_file[[\"محتوا\"]].values\n",
    "\n",
    "y8_train = dataset[[\"موضوع\"]].values\n",
    "y8_test =  test_file[[\"موضوع\"]].values\n",
    "y9_train = dataset[[\"کارگردان\"]].values\n",
    "y9_test =  test_file[[\"کارگردان\"]].values\n",
    "\n",
    "#شمارش تعداد کلمات منحصر به فرد\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "all_words=' '.join(data)\n",
    "all_words=word_tokenize(all_words)\n",
    "dist=FreqDist(all_words)\n",
    "num_unique_word=len(dist)\n",
    "print ('number unique word:',num_unique_word)\n",
    "#شمارش بزرگترین طول متن\n",
    "r_len=[]\n",
    "for text in data:\n",
    "    word=word_tokenize(text)\n",
    "    l=len(word)\n",
    "    r_len.append(l)   \n",
    "MAX_REVIEW_LEN=np.max(r_len)\n",
    "print('max len:',MAX_REVIEW_LEN)\n",
    "np.random.seed(sd)\n",
    "num_words = num_unique_word\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "np.random.seed(sd)\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)\n",
    "print(x_train[100])\n",
    "print(x_train_tokens[100])\n",
    "max_tokens = MAX_REVIEW_LEN\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,padding='post')\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,padding='post')\n",
    "np.random.seed(sd)\n",
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
    "def tokens_to_string(tokens):\n",
    "    words = [inverse_map[token] for token in tokens if token!=0]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "print('train shape',x_train_pad.shape)\n",
    "print('test shape',x_test_pad.shape)\n",
    "np.random.seed(sd)\n",
    "#طراحی مدل\n",
    "embedding_size = 300\n",
    "input_1 = Input(shape=(max_tokens,))\n",
    "x=Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    name='embedding_layer',\n",
    "            embeddings_initializer=initializer)(input_1)\n",
    "x=Dropout(0.2,seed=sd)(x)\n",
    "x=Conv1D(256,kernel_size=3,padding='same',activation='relu',strides=1\n",
    "       ,kernel_initializer=initializer,use_bias=False)(x)\n",
    "x=BatchNormalization()(x)\n",
    "x=GlobalMaxPooling1D()(x)\n",
    "\n",
    "x=Dense(200, activation='relu',\n",
    "        kernel_initializer=initializer,use_bias=False)(x)\n",
    "x=Dropout(0.2,seed=sd)(x)\n",
    "output1 = Dense(3, activation='softmax',name='bazigar')(x)\n",
    "output2 = Dense(3, activation='softmax',name='bazigari')(x)\n",
    "output3 = Dense(3, activation='softmax',name='dastan')(x)\n",
    "output4 = Dense(3, activation='softmax',name='dialog')(x)\n",
    "output5 = Dense(3, activation='softmax')(x)\n",
    "output6 = Dense(3, activation='softmax')(x)\n",
    "output7 = Dense(3, activation='softmax')(x)\n",
    "output8 = Dense(3, activation='softmax')(x)\n",
    "output9 = Dense(3, activation='softmax')(x)\n",
    "model = Model(inputs=input_1, outputs=[output1, output2, output3, output4, output5, output6,output7,output8,output9\n",
    "                                      ])\n",
    "\n",
    "optimizer =Nadam(learning_rate=1e-3)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizer,)\n",
    "model.summary()\n",
    "np.random.seed(sd)\n",
    "history=model.fit(x_train_pad, \n",
    "                   y=[y1_train,y2_train,y3_train,y4_train,\n",
    "                                  y5_train,y6_train,y7_train,\n",
    "                                  y8_train,y9_train]\n",
    "                   , epochs=20, batch_size=256,shuffle=False,callbacks=[tensorboard])\n",
    "\n",
    "predicted = model.predict(x_test_pad)\n",
    "pred_class = np.argmax(predicted, axis=-1) \n",
    "from sklearn.metrics import hamming_loss, accuracy_score,precision_score\n",
    "p=pred_class.T\n",
    "hm=[]\n",
    "ac=[]\n",
    "x1=[]\n",
    "x2=[]\n",
    "for i in range(len(y_test)):\n",
    "  hm.append(hamming_loss(y_test[i], p[i]))\n",
    "print(np.mean(hm))\n",
    "print (jacard(y_test, p))\n",
    "print (subsetAccuracy(y_test, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EX7AbMxyLG7y",
    "outputId": "79794205-57ad-4c82-fccd-0484fd5a6f52"
   },
   "outputs": [],
   "source": [
    "#تکرار پذیری مدل\n",
    "import os\n",
    "sd = 1024\n",
    "os.environ['PYTHONHASHSEED']=str(sd)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import random as rn\n",
    "random.seed(sd)\n",
    "np.random.seed(sd)\n",
    "rn.seed(sd)\n",
    "from keras import backend as K\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "tf.random.set_seed(sd)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "tf.compat.v1.keras.backend.set_floatx('float64')\n",
    "initializer = tf.random_normal_initializer(0., 0.02, seed=sd)\n",
    "#فراخوانی کتابخانه ها\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras.models import Model,Sequential\n",
    "from tensorflow.python.keras.layers import Input,Dense, GRU, Embedding, LSTM,Conv1D\n",
    "from tensorflow.keras.optimizers import Adam,Adagrad,Nadam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dropout,GlobalMaxPooling1D,Conv1D,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tensorboard = TensorBoard(log_dir='drive/My Drive/logs/{}'.format(time()))\n",
    "\n",
    "#فراخوانی داده های آموزشی و آزمایشی\n",
    "dataset = pd.read_excel('/content/drive/MyDrive/تحلیل احساسات سطح ویژگی/train.xlsx')\n",
    "test_file = pd.read_excel('/content/drive/MyDrive/تحلیل احساسات سطح ویژگی/test.xlsx')\n",
    "S_comments_labels = dataset[['بازیگر','بازیگری','داستان',\n",
    "                              'سبک','فیلم',\n",
    "                             'فیلم_نامه',\n",
    "                             'محتوا',\n",
    "                            'موضوع','کارگردان',\n",
    "                            ]]\n",
    "\n",
    "S_test_comments_labels = test_file[['بازیگر','بازیگری','داستان',\n",
    "                              'سبک','فیلم',\n",
    "                             'فیلم_نامه',\n",
    "                             'محتوا',\n",
    "                            'موضوع','کارگردان',\n",
    "                            ]]\n",
    "category_list=['بازیگر','بازیگری','داستان',\n",
    "                              'سبک','فیلم',\n",
    "                             'فیلم_نامه',\n",
    "                             'محتوا',\n",
    "                            'موضوع','کارگردان',\n",
    "                            ]\n",
    "stop_words =['به','شکل','ممکن','میشه','دلیل','رو','فقط','در','خانم','آقای','اگر','مریلا',\n",
    "            'این','هم','اسامی','زارعی','بهرام','رادان','حیایی','پارسا','پیروزفر','و','صابر','ابر','حال','های',\n",
    "            'ها','یه','ای','بابت','آبیار','از','تا','هفتاد','درصد','الناز','دو','اول','شاکردوست','سیمرغ','زن','تاریخ','شاکر','هاش','ریگی','اش','معادی','نرگس','دوم','ابیار','پیمان','اقای','بهداد','مهناز','حامد','عطاران','علی','نوید','ام','امیر','۱۰','محمدزاده','نفر','تیکت','سعید','لباس','خدا','هانیه'\n",
    "             ,'توسلی','سحر','۲۰','ثقفی','رضا','کریمی','روسی','۴','کامنت','عبدالمالک','ش','اکبر','هیس','مهرجویی','طباطبایی','سرخپوست','عبدی','شش','شیطان','پرستی'\n",
    "            ]\n",
    "#پیش پردازش\n",
    "import re\n",
    "def preprocess_text(sentence):\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'@\\S+', '', sentence)\n",
    "    sentence = re.sub(r'!\\S+', '', sentence)\n",
    "    sentence = re.sub(r'؟\\S+', '', sentence)\n",
    "    sentence = re.sub(r'[.]', ' ', sentence)\n",
    "    sentence = re.sub(r'[/]', '', sentence)\n",
    "    sentence = re.sub(r'[،]', ' ', sentence)\n",
    "    sentence = re.sub(r'[؛]', '', sentence)\n",
    "    sentence = sentence.split()\n",
    "    sentence =[word for word in sentence if word not in stop_words]\n",
    "    sentence = ' '.join(sentence)\n",
    "    return sentence\n",
    "def ReplacetwoMore(s):\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\", s)\n",
    "dataset['clean_text'] = dataset['Sentence'].apply(ReplacetwoMore)\n",
    "test_file['clean_text'] = test_file['Sentence'].apply(ReplacetwoMore)\n",
    "X = []\n",
    "sentences = list(dataset[\"clean_text\"])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))    \n",
    "X_t = []\n",
    "sentences2 = list(test_file[\"clean_text\"])\n",
    "for sen in sentences2:\n",
    "    X_t.append(preprocess_text(sen))\n",
    "np.random.seed(sd)\n",
    "\n",
    "y = S_comments_labels.values\n",
    "y_test1=S_test_comments_labels.values\n",
    "target =y\n",
    "data = X\n",
    "target1 = y_test1\n",
    "data1 = X_t\n",
    "x_train, x_test = data, data1\n",
    "y_train, y_test = target, target1\n",
    "np.random.seed(sd)\n",
    "print(x_train[136])\n",
    "print(y_train[136])\n",
    "y1_train = dataset[[\"بازیگر\"]].values\n",
    "y1_test =  test_file[[\"بازیگر\"]].values\n",
    "y2_train = dataset[[\"بازیگری\"]].values\n",
    "y2_test =  test_file[[\"بازیگری\"]].values\n",
    "y3_train = dataset[[\"داستان\"]].values\n",
    "y3_test =  test_file[[\"داستان\"]].values\n",
    "\n",
    "y4_train = dataset[[\"سبک\"]].values\n",
    "y4_test =  test_file[[\"سبک\"]].values\n",
    "y5_train = dataset[[\"فیلم\"]].values\n",
    "y5_test =  test_file[[\"فیلم\"]].values\n",
    "y6_train = dataset[[\"فیلم_نامه\"]].values\n",
    "y6_test =  test_file[[\"فیلم_نامه\"]].values\n",
    "\n",
    "\n",
    "y7_train = dataset[[\"محتوا\"]].values\n",
    "y7_test =  test_file[[\"محتوا\"]].values\n",
    "\n",
    "y8_train = dataset[[\"موضوع\"]].values\n",
    "y8_test =  test_file[[\"موضوع\"]].values\n",
    "y9_train = dataset[[\"کارگردان\"]].values\n",
    "y9_test =  test_file[[\"کارگردان\"]].values\n",
    "\n",
    "#شمارش تعداد کلمات منحصر به فرد\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "all_words=' '.join(data)\n",
    "all_words=word_tokenize(all_words)\n",
    "dist=FreqDist(all_words)\n",
    "num_unique_word=len(dist)\n",
    "print ('number unique word:',num_unique_word)\n",
    "#شمارش بزرگترین طول متن\n",
    "r_len=[]\n",
    "for text in data:\n",
    "    word=word_tokenize(text)\n",
    "    l=len(word)\n",
    "    r_len.append(l)   \n",
    "MAX_REVIEW_LEN=np.max(r_len)\n",
    "print('max len:',MAX_REVIEW_LEN)\n",
    "np.random.seed(sd)\n",
    "num_words = num_unique_word\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "np.random.seed(sd)\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)\n",
    "print(x_train[100])\n",
    "print(x_train_tokens[100])\n",
    "max_tokens = MAX_REVIEW_LEN\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,padding='post')\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,padding='post')\n",
    "np.random.seed(sd)\n",
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
    "def tokens_to_string(tokens):\n",
    "    words = [inverse_map[token] for token in tokens if token!=0]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "print('train shape',x_train_pad.shape)\n",
    "print('test shape',x_test_pad.shape)\n",
    "np.random.seed(sd)\n",
    "#طراحی مدل\n",
    "embedding_size = 300\n",
    "input_1 = Input(shape=(max_tokens,))\n",
    "x=Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    name='embedding_layer',\n",
    "            embeddings_initializer=initializer)(input_1)\n",
    "x=Dropout(0.2,seed=sd)(x)\n",
    "x=LSTM(units=256, return_sequences=True,kernel_initializer=initializer,use_bias=False)(x)\n",
    "x=BatchNormalization()(x)\n",
    "x=GlobalMaxPooling1D()(x)\n",
    "x=Dense(200, activation='relu',\n",
    "        kernel_initializer=initializer,use_bias=False)(x)\n",
    "x=Dropout(0.2,seed=sd)(x)\n",
    "output1 = Dense(3, activation='softmax',name='bazigar')(x)\n",
    "output2 = Dense(3, activation='softmax',name='bazigari')(x)\n",
    "output3 = Dense(3, activation='softmax',name='dastan')(x)\n",
    "output4 = Dense(3, activation='softmax',name='dialog')(x)\n",
    "output5 = Dense(3, activation='softmax')(x)\n",
    "output6 = Dense(3, activation='softmax')(x)\n",
    "output7 = Dense(3, activation='softmax')(x)\n",
    "output8 = Dense(3, activation='softmax')(x)\n",
    "output9 = Dense(3, activation='softmax')(x)\n",
    "model = Model(inputs=input_1, outputs=[output1, output2, output3, output4, output5, output6,output7,output8,output9\n",
    "                                      ])\n",
    "\n",
    "optimizer =Nadam(learning_rate=1e-3)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizer,)\n",
    "model.summary()\n",
    "np.random.seed(sd)\n",
    "history=model.fit(x_train_pad, \n",
    "                   y=[y1_train,y2_train,y3_train,y4_train,\n",
    "                                  y5_train,y6_train,y7_train,\n",
    "                                  y8_train,y9_train]\n",
    "                   , epochs=20, batch_size=256,shuffle=False,callbacks=[tensorboard])\n",
    "\n",
    "predicted = model.predict(x_test_pad)\n",
    "pred_class = np.argmax(predicted, axis=-1) \n",
    "from sklearn.metrics import hamming_loss, accuracy_score,precision_score\n",
    "p=pred_class.T\n",
    "hm=[]\n",
    "ac=[]\n",
    "x1=[]\n",
    "x2=[]\n",
    "for i in range(len(y_test)):\n",
    "  hm.append(hamming_loss(y_test[i], p[i]))\n",
    "print(np.mean(hm))\n",
    "print (jacard(y_test, p))\n",
    "print (subsetAccuracy(y_test, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1vB4qHaTwb3",
    "outputId": "55350a5b-7b0c-47a0-ae33-fc030dce5ba7"
   },
   "outputs": [],
   "source": [
    "#تکرار پذیری مدل\n",
    "import os\n",
    "sd = 1024\n",
    "os.environ['PYTHONHASHSEED']=str(sd)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import random as rn\n",
    "random.seed(sd)\n",
    "np.random.seed(sd)\n",
    "rn.seed(sd)\n",
    "from keras import backend as K\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "tf.random.set_seed(sd)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "tf.compat.v1.keras.backend.set_floatx('float64')\n",
    "initializer = tf.random_normal_initializer(0., 0.02, seed=sd)\n",
    "#فراخوانی کتابخانه ها\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras.models import Model,Sequential\n",
    "from tensorflow.python.keras.layers import Input,Dense, GRU, Embedding, LSTM,Conv1D\n",
    "from tensorflow.keras.optimizers import Adam,Adagrad,Nadam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dropout,GlobalMaxPooling1D,Conv1D,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tensorboard = TensorBoard(log_dir='drive/My Drive/logs/{}'.format(time()))\n",
    "\n",
    "#فراخوانی داده های آموزشی و آزمایشی\n",
    "dataset = pd.read_excel('/content/drive/MyDrive/تحلیل احساسات سطح ویژگی/train.xlsx')\n",
    "test_file = pd.read_excel('/content/drive/MyDrive/تحلیل احساسات سطح ویژگی/test.xlsx')\n",
    "S_comments_labels = dataset[['بازیگر','بازیگری','داستان',\n",
    "                              'سبک','فیلم',\n",
    "                             'فیلم_نامه',\n",
    "                             'محتوا',\n",
    "                            'موضوع','کارگردان',\n",
    "                            ]]\n",
    "\n",
    "S_test_comments_labels = test_file[['بازیگر','بازیگری','داستان',\n",
    "                              'سبک','فیلم',\n",
    "                             'فیلم_نامه',\n",
    "                             'محتوا',\n",
    "                            'موضوع','کارگردان',\n",
    "                            ]]\n",
    "category_list=['بازیگر','بازیگری','داستان',\n",
    "                              'سبک','فیلم',\n",
    "                             'فیلم_نامه',\n",
    "                             'محتوا',\n",
    "                            'موضوع','کارگردان',\n",
    "                            ]\n",
    "stop_words =['به','شکل','ممکن','میشه','دلیل','رو','فقط','در','خانم','آقای','اگر','مریلا',\n",
    "            'این','هم','اسامی','زارعی','بهرام','رادان','حیایی','پارسا','پیروزفر','و','صابر','ابر','حال','های',\n",
    "            'ها','یه','ای','بابت','آبیار','از','تا','هفتاد','درصد','الناز','دو','اول','شاکردوست','سیمرغ','زن','تاریخ','شاکر','هاش','ریگی','اش','معادی','نرگس','دوم','ابیار','پیمان','اقای','بهداد','مهناز','حامد','عطاران','علی','نوید','ام','امیر','۱۰','محمدزاده','نفر','تیکت','سعید','لباس','خدا','هانیه'\n",
    "             ,'توسلی','سحر','۲۰','ثقفی','رضا','کریمی','روسی','۴','کامنت','عبدالمالک','ش','اکبر','هیس','مهرجویی','طباطبایی','سرخپوست','عبدی','شش','شیطان','پرستی'\n",
    "            ]\n",
    "#پیش پردازش\n",
    "import re\n",
    "def preprocess_text(sentence):\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'@\\S+', '', sentence)\n",
    "    sentence = re.sub(r'!\\S+', '', sentence)\n",
    "    sentence = re.sub(r'؟\\S+', '', sentence)\n",
    "    sentence = re.sub(r'[.]', ' ', sentence)\n",
    "    sentence = re.sub(r'[/]', '', sentence)\n",
    "    sentence = re.sub(r'[،]', ' ', sentence)\n",
    "    sentence = re.sub(r'[؛]', '', sentence)\n",
    "    sentence = sentence.split()\n",
    "    sentence =[word for word in sentence if word not in stop_words]\n",
    "    sentence = ' '.join(sentence)\n",
    "    return sentence\n",
    "def ReplacetwoMore(s):\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\", s)\n",
    "dataset['clean_text'] = dataset['Sentence'].apply(ReplacetwoMore)\n",
    "test_file['clean_text'] = test_file['Sentence'].apply(ReplacetwoMore)\n",
    "X = []\n",
    "sentences = list(dataset[\"clean_text\"])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))    \n",
    "X_t = []\n",
    "sentences2 = list(test_file[\"clean_text\"])\n",
    "for sen in sentences2:\n",
    "    X_t.append(preprocess_text(sen))\n",
    "np.random.seed(sd)\n",
    "\n",
    "y = S_comments_labels.values\n",
    "y_test1=S_test_comments_labels.values\n",
    "target =y\n",
    "data = X\n",
    "target1 = y_test1\n",
    "data1 = X_t\n",
    "x_train, x_test = data, data1\n",
    "y_train, y_test = target, target1\n",
    "np.random.seed(sd)\n",
    "print(x_train[136])\n",
    "print(y_train[136])\n",
    "y1_train = dataset[[\"بازیگر\"]].values\n",
    "y1_test =  test_file[[\"بازیگر\"]].values\n",
    "y2_train = dataset[[\"بازیگری\"]].values\n",
    "y2_test =  test_file[[\"بازیگری\"]].values\n",
    "y3_train = dataset[[\"داستان\"]].values\n",
    "y3_test =  test_file[[\"داستان\"]].values\n",
    "\n",
    "y4_train = dataset[[\"سبک\"]].values\n",
    "y4_test =  test_file[[\"سبک\"]].values\n",
    "y5_train = dataset[[\"فیلم\"]].values\n",
    "y5_test =  test_file[[\"فیلم\"]].values\n",
    "y6_train = dataset[[\"فیلم_نامه\"]].values\n",
    "y6_test =  test_file[[\"فیلم_نامه\"]].values\n",
    "\n",
    "\n",
    "y7_train = dataset[[\"محتوا\"]].values\n",
    "y7_test =  test_file[[\"محتوا\"]].values\n",
    "\n",
    "y8_train = dataset[[\"موضوع\"]].values\n",
    "y8_test =  test_file[[\"موضوع\"]].values\n",
    "y9_train = dataset[[\"کارگردان\"]].values\n",
    "y9_test =  test_file[[\"کارگردان\"]].values\n",
    "\n",
    "#شمارش تعداد کلمات منحصر به فرد\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "all_words=' '.join(data)\n",
    "all_words=word_tokenize(all_words)\n",
    "dist=FreqDist(all_words)\n",
    "num_unique_word=len(dist)\n",
    "print ('number unique word:',num_unique_word)\n",
    "#شمارش بزرگترین طول متن\n",
    "r_len=[]\n",
    "for text in data:\n",
    "    word=word_tokenize(text)\n",
    "    l=len(word)\n",
    "    r_len.append(l)   \n",
    "MAX_REVIEW_LEN=np.max(r_len)\n",
    "print('max len:',MAX_REVIEW_LEN)\n",
    "np.random.seed(sd)\n",
    "num_words = num_unique_word\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "np.random.seed(sd)\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)\n",
    "print(x_train[100])\n",
    "print(x_train_tokens[100])\n",
    "max_tokens = MAX_REVIEW_LEN\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,padding='post')\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,padding='post')\n",
    "np.random.seed(sd)\n",
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
    "def tokens_to_string(tokens):\n",
    "    words = [inverse_map[token] for token in tokens if token!=0]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "print('train shape',x_train_pad.shape)\n",
    "print('test shape',x_test_pad.shape)\n",
    "np.random.seed(sd)\n",
    "#طراحی مدل\n",
    "embedding_size = 300\n",
    "input_1 = Input(shape=(max_tokens,))\n",
    "x=Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    name='embedding_layer',\n",
    "            embeddings_initializer=initializer)(input_1)\n",
    "x=Dropout(0.2,seed=sd)(x)\n",
    "x=GRU(units=256, return_sequences=True,kernel_initializer=initializer,use_bias=False)(x)\n",
    "x=BatchNormalization()(x)\n",
    "x=GlobalMaxPooling1D()(x)\n",
    "x=Dense(200, activation='relu',\n",
    "        kernel_initializer=initializer,use_bias=False)(x)\n",
    "x=Dropout(0.2,seed=sd)(x)\n",
    "output1 = Dense(3, activation='softmax',name='bazigar')(x)\n",
    "output2 = Dense(3, activation='softmax',name='bazigari')(x)\n",
    "output3 = Dense(3, activation='softmax',name='dastan')(x)\n",
    "output4 = Dense(3, activation='softmax',name='dialog')(x)\n",
    "output5 = Dense(3, activation='softmax')(x)\n",
    "output6 = Dense(3, activation='softmax')(x)\n",
    "output7 = Dense(3, activation='softmax')(x)\n",
    "output8 = Dense(3, activation='softmax')(x)\n",
    "output9 = Dense(3, activation='softmax')(x)\n",
    "model = Model(inputs=input_1, outputs=[output1, output2, output3, output4, output5, output6,output7,output8,output9\n",
    "                                      ])\n",
    "\n",
    "optimizer =Nadam(learning_rate=1e-3)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizer,)\n",
    "model.summary()\n",
    "np.random.seed(sd)\n",
    "history=model.fit(x_train_pad, \n",
    "                   y=[y1_train,y2_train,y3_train,y4_train,\n",
    "                                  y5_train,y6_train,y7_train,\n",
    "                                  y8_train,y9_train]\n",
    "                   , epochs=20, batch_size=256,shuffle=False,callbacks=[tensorboard])\n",
    "\n",
    "predicted = model.predict(x_test_pad)\n",
    "pred_class = np.argmax(predicted, axis=-1) \n",
    "from sklearn.metrics import hamming_loss, accuracy_score,precision_score\n",
    "p=pred_class.T\n",
    "hm=[]\n",
    "ac=[]\n",
    "x1=[]\n",
    "x2=[]\n",
    "for i in range(len(y_test)):\n",
    "  hm.append(hamming_loss(y_test[i], p[i]))\n",
    "print(np.mean(hm))\n",
    "print (jacard(y_test, p))\n",
    "print (subsetAccuracy(y_test, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5PoAnXJT8Pe",
    "outputId": "cd706156-e3e5-4fc3-b5b4-60777263567b"
   },
   "outputs": [],
   "source": [
    "#تکرار پذیری مدل\n",
    "import os\n",
    "sd = 1024\n",
    "os.environ['PYTHONHASHSEED']=str(sd)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import random as rn\n",
    "random.seed(sd)\n",
    "np.random.seed(sd)\n",
    "rn.seed(sd)\n",
    "from keras import backend as K\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "tf.random.set_seed(sd)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "tf.compat.v1.keras.backend.set_floatx('float64')\n",
    "initializer = tf.random_normal_initializer(0., 0.02, seed=sd)\n",
    "#فراخوانی کتابخانه ها\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras.models import Model,Sequential\n",
    "from tensorflow.python.keras.layers import Input,Dense, GRU, Embedding, LSTM,Conv1D,Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam,Adagrad,Nadam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dropout,GlobalMaxPooling1D,Conv1D,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tensorboard = TensorBoard(log_dir='drive/My Drive/logs/{}'.format(time()))\n",
    "\n",
    "#فراخوانی داده های آموزشی و آزمایشی\n",
    "dataset = pd.read_excel('/content/drive/MyDrive/تحلیل احساسات سطح ویژگی/train.xlsx')\n",
    "test_file = pd.read_excel('/content/drive/MyDrive/تحلیل احساسات سطح ویژگی/test.xlsx')\n",
    "S_comments_labels = dataset[['بازیگر','بازیگری','داستان',\n",
    "                              'سبک','فیلم',\n",
    "                             'فیلم_نامه',\n",
    "                             'محتوا',\n",
    "                            'موضوع','کارگردان',\n",
    "                            ]]\n",
    "\n",
    "S_test_comments_labels = test_file[['بازیگر','بازیگری','داستان',\n",
    "                              'سبک','فیلم',\n",
    "                             'فیلم_نامه',\n",
    "                             'محتوا',\n",
    "                            'موضوع','کارگردان',\n",
    "                            ]]\n",
    "category_list=['بازیگر','بازیگری','داستان',\n",
    "                              'سبک','فیلم',\n",
    "                             'فیلم_نامه',\n",
    "                             'محتوا',\n",
    "                            'موضوع','کارگردان',\n",
    "                            ]\n",
    "stop_words =['به','شکل','ممکن','میشه','دلیل','رو','فقط','در','خانم','آقای','اگر','مریلا',\n",
    "            'این','هم','اسامی','زارعی','بهرام','رادان','حیایی','پارسا','پیروزفر','و','صابر','ابر','حال','های',\n",
    "            'ها','یه','ای','بابت','آبیار','از','تا','هفتاد','درصد','الناز','دو','اول','شاکردوست','سیمرغ','زن','تاریخ','شاکر','هاش','ریگی','اش','معادی','نرگس','دوم','ابیار','پیمان','اقای','بهداد','مهناز','حامد','عطاران','علی','نوید','ام','امیر','۱۰','محمدزاده','نفر','تیکت','سعید','لباس','خدا','هانیه'\n",
    "             ,'توسلی','سحر','۲۰','ثقفی','رضا','کریمی','روسی','۴','کامنت','عبدالمالک','ش','اکبر','هیس','مهرجویی','طباطبایی','سرخپوست','عبدی','شش','شیطان','پرستی'\n",
    "            ]\n",
    "#پیش پردازش\n",
    "import re\n",
    "def preprocess_text(sentence):\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'@\\S+', '', sentence)\n",
    "    sentence = re.sub(r'!\\S+', '', sentence)\n",
    "    sentence = re.sub(r'؟\\S+', '', sentence)\n",
    "    sentence = re.sub(r'[.]', ' ', sentence)\n",
    "    sentence = re.sub(r'[/]', '', sentence)\n",
    "    sentence = re.sub(r'[،]', ' ', sentence)\n",
    "    sentence = re.sub(r'[؛]', '', sentence)\n",
    "    sentence = sentence.split()\n",
    "    sentence =[word for word in sentence if word not in stop_words]\n",
    "    sentence = ' '.join(sentence)\n",
    "    return sentence\n",
    "def ReplacetwoMore(s):\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\", s)\n",
    "dataset['clean_text'] = dataset['Sentence'].apply(ReplacetwoMore)\n",
    "test_file['clean_text'] = test_file['Sentence'].apply(ReplacetwoMore)\n",
    "X = []\n",
    "sentences = list(dataset[\"clean_text\"])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))    \n",
    "X_t = []\n",
    "sentences2 = list(test_file[\"clean_text\"])\n",
    "for sen in sentences2:\n",
    "    X_t.append(preprocess_text(sen))\n",
    "np.random.seed(sd)\n",
    "\n",
    "y = S_comments_labels.values\n",
    "y_test1=S_test_comments_labels.values\n",
    "target =y\n",
    "data = X\n",
    "target1 = y_test1\n",
    "data1 = X_t\n",
    "x_train, x_test = data, data1\n",
    "y_train, y_test = target, target1\n",
    "np.random.seed(sd)\n",
    "print(x_train[136])\n",
    "print(y_train[136])\n",
    "y1_train = dataset[[\"بازیگر\"]].values\n",
    "y1_test =  test_file[[\"بازیگر\"]].values\n",
    "y2_train = dataset[[\"بازیگری\"]].values\n",
    "y2_test =  test_file[[\"بازیگری\"]].values\n",
    "y3_train = dataset[[\"داستان\"]].values\n",
    "y3_test =  test_file[[\"داستان\"]].values\n",
    "\n",
    "y4_train = dataset[[\"سبک\"]].values\n",
    "y4_test =  test_file[[\"سبک\"]].values\n",
    "y5_train = dataset[[\"فیلم\"]].values\n",
    "y5_test =  test_file[[\"فیلم\"]].values\n",
    "y6_train = dataset[[\"فیلم_نامه\"]].values\n",
    "y6_test =  test_file[[\"فیلم_نامه\"]].values\n",
    "\n",
    "\n",
    "y7_train = dataset[[\"محتوا\"]].values\n",
    "y7_test =  test_file[[\"محتوا\"]].values\n",
    "\n",
    "y8_train = dataset[[\"موضوع\"]].values\n",
    "y8_test =  test_file[[\"موضوع\"]].values\n",
    "y9_train = dataset[[\"کارگردان\"]].values\n",
    "y9_test =  test_file[[\"کارگردان\"]].values\n",
    "\n",
    "#شمارش تعداد کلمات منحصر به فرد\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "all_words=' '.join(data)\n",
    "all_words=word_tokenize(all_words)\n",
    "dist=FreqDist(all_words)\n",
    "num_unique_word=len(dist)\n",
    "print ('number unique word:',num_unique_word)\n",
    "#شمارش بزرگترین طول متن\n",
    "r_len=[]\n",
    "for text in data:\n",
    "    word=word_tokenize(text)\n",
    "    l=len(word)\n",
    "    r_len.append(l)   \n",
    "MAX_REVIEW_LEN=np.max(r_len)\n",
    "print('max len:',MAX_REVIEW_LEN)\n",
    "np.random.seed(sd)\n",
    "num_words = num_unique_word\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "np.random.seed(sd)\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)\n",
    "print(x_train[100])\n",
    "print(x_train_tokens[100])\n",
    "max_tokens = MAX_REVIEW_LEN\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,padding='post')\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,padding='post')\n",
    "np.random.seed(sd)\n",
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
    "def tokens_to_string(tokens):\n",
    "    words = [inverse_map[token] for token in tokens if token!=0]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "print('train shape',x_train_pad.shape)\n",
    "print('test shape',x_test_pad.shape)\n",
    "np.random.seed(sd)\n",
    "#طراحی مدل\n",
    "embedding_size = 300\n",
    "input_1 = Input(shape=(max_tokens,))\n",
    "x=Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    name='embedding_layer',\n",
    "            embeddings_initializer=initializer)(input_1)\n",
    "x=Dropout(0.2,seed=sd)(x)\n",
    "x=Bidirectional(LSTM(units=256, return_sequences=True,kernel_initializer=initializer,use_bias=False))(x)\n",
    "x=BatchNormalization()(x)\n",
    "x=GlobalMaxPooling1D()(x)\n",
    "x=Dense(200, activation='relu',\n",
    "        kernel_initializer=initializer,use_bias=False)(x)\n",
    "x=Dropout(0.2,seed=sd)(x)\n",
    "output1 = Dense(3, activation='softmax',name='bazigar')(x)\n",
    "output2 = Dense(3, activation='softmax',name='bazigari')(x)\n",
    "output3 = Dense(3, activation='softmax',name='dastan')(x)\n",
    "output4 = Dense(3, activation='softmax',name='dialog')(x)\n",
    "output5 = Dense(3, activation='softmax')(x)\n",
    "output6 = Dense(3, activation='softmax')(x)\n",
    "output7 = Dense(3, activation='softmax')(x)\n",
    "output8 = Dense(3, activation='softmax')(x)\n",
    "output9 = Dense(3, activation='softmax')(x)\n",
    "model = Model(inputs=input_1, outputs=[output1, output2, output3, output4, output5, output6,output7,output8,output9\n",
    "                                      ])\n",
    "\n",
    "optimizer =Nadam(learning_rate=1e-3)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizer,)\n",
    "model.summary()\n",
    "np.random.seed(sd)\n",
    "history=model.fit(x_train_pad, \n",
    "                   y=[y1_train,y2_train,y3_train,y4_train,\n",
    "                                  y5_train,y6_train,y7_train,\n",
    "                                  y8_train,y9_train]\n",
    "                   , epochs=20, batch_size=256,shuffle=False,callbacks=[tensorboard])\n",
    "\n",
    "predicted = model.predict(x_test_pad)\n",
    "pred_class = np.argmax(predicted, axis=-1) \n",
    "from sklearn.metrics import hamming_loss, accuracy_score,precision_score\n",
    "p=pred_class.T\n",
    "hm=[]\n",
    "ac=[]\n",
    "x1=[]\n",
    "x2=[]\n",
    "for i in range(len(y_test)):\n",
    "  hm.append(hamming_loss(y_test[i], p[i]))\n",
    "print(np.mean(hm))\n",
    "print (jacard(y_test, p))\n",
    "print (subsetAccuracy(y_test, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJniY2NmYUsT",
    "outputId": "9fbb1898-1036-4a1d-cf96-58263916422d"
   },
   "outputs": [],
   "source": [
    "!pip install attention\n",
    "from attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAZmLHXbYPO4",
    "outputId": "428af9a3-fd2d-4e99-dd77-5c41d7d51701"
   },
   "outputs": [],
   "source": [
    "#تکرار پذیری مدل\n",
    "import os\n",
    "sd = 1024\n",
    "os.environ['PYTHONHASHSEED']=str(sd)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import random as rn\n",
    "random.seed(sd)\n",
    "np.random.seed(sd)\n",
    "rn.seed(sd)\n",
    "from keras import backend as K\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "tf.random.set_seed(sd)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "tf.compat.v1.keras.backend.set_floatx('float64')\n",
    "initializer = tf.random_normal_initializer(0., 0.02, seed=sd)\n",
    "#فراخوانی کتابخانه ها\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras.models import Model,Sequential\n",
    "from tensorflow.python.keras.layers import Input,Dense, GRU, Embedding, LSTM,Conv1D,Bidirectional,Flatten\n",
    "from tensorflow.keras.optimizers import Adam,Adagrad,Nadam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dropout,GlobalMaxPooling1D,Conv1D,BatchNormalization,MaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tensorboard = TensorBoard(log_dir='drive/My Drive/logs/{}'.format(time()))\n",
    "\n",
    "#فراخوانی داده های آموزشی و آزمایشی\n",
    "dataset = pd.read_excel('/content/drive/MyDrive/تحلیل احساسات سطح ویژگی/train.xlsx')\n",
    "test_file = pd.read_excel('/content/drive/MyDrive/تحلیل احساسات سطح ویژگی/test.xlsx')\n",
    "S_comments_labels = dataset[['بازیگر','بازیگری','داستان',\n",
    "                              'سبک','فیلم',\n",
    "                             'فیلم_نامه',\n",
    "                             'محتوا',\n",
    "                            'موضوع','کارگردان',\n",
    "                            ]]\n",
    "\n",
    "S_test_comments_labels = test_file[['بازیگر','بازیگری','داستان',\n",
    "                              'سبک','فیلم',\n",
    "                             'فیلم_نامه',\n",
    "                             'محتوا',\n",
    "                            'موضوع','کارگردان',\n",
    "                            ]]\n",
    "category_list=['بازیگر','بازیگری','داستان',\n",
    "                              'سبک','فیلم',\n",
    "                             'فیلم_نامه',\n",
    "                             'محتوا',\n",
    "                            'موضوع','کارگردان',\n",
    "                            ]\n",
    "stop_words =['به','شکل','ممکن','میشه','دلیل','رو','فقط','در','خانم','آقای','اگر','مریلا',\n",
    "            'این','هم','اسامی','زارعی','بهرام','رادان','حیایی','پارسا','پیروزفر','و','صابر','ابر','حال','های',\n",
    "            'ها','یه','ای','بابت','آبیار','از','تا','هفتاد','درصد','الناز','دو','اول','شاکردوست','سیمرغ','زن','تاریخ','شاکر','هاش','ریگی','اش','معادی','نرگس','دوم','ابیار','پیمان','اقای','بهداد','مهناز','حامد','عطاران','علی','نوید','ام','امیر','۱۰','محمدزاده','نفر','تیکت','سعید','لباس','خدا','هانیه'\n",
    "             ,'توسلی','سحر','۲۰','ثقفی','رضا','کریمی','روسی','۴','کامنت','عبدالمالک','ش','اکبر','هیس','مهرجویی','طباطبایی','سرخپوست','عبدی','شش','شیطان','پرستی'\n",
    "            ]\n",
    "#پیش پردازش\n",
    "import re\n",
    "def preprocess_text(sentence):\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'@\\S+', '', sentence)\n",
    "    sentence = re.sub(r'!\\S+', '', sentence)\n",
    "    sentence = re.sub(r'؟\\S+', '', sentence)\n",
    "    sentence = re.sub(r'[.]', ' ', sentence)\n",
    "    sentence = re.sub(r'[/]', '', sentence)\n",
    "    sentence = re.sub(r'[،]', ' ', sentence)\n",
    "    sentence = re.sub(r'[؛]', '', sentence)\n",
    "    sentence = sentence.split()\n",
    "    sentence =[word for word in sentence if word not in stop_words]\n",
    "    sentence = ' '.join(sentence)\n",
    "    return sentence\n",
    "def ReplacetwoMore(s):\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\", s)\n",
    "dataset['clean_text'] = dataset['Sentence'].apply(ReplacetwoMore)\n",
    "test_file['clean_text'] = test_file['Sentence'].apply(ReplacetwoMore)\n",
    "X = []\n",
    "sentences = list(dataset[\"clean_text\"])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))    \n",
    "X_t = []\n",
    "sentences2 = list(test_file[\"clean_text\"])\n",
    "for sen in sentences2:\n",
    "    X_t.append(preprocess_text(sen))\n",
    "np.random.seed(sd)\n",
    "\n",
    "y = S_comments_labels.values\n",
    "y_test1=S_test_comments_labels.values\n",
    "target =y\n",
    "data = X\n",
    "target1 = y_test1\n",
    "data1 = X_t\n",
    "x_train, x_test = data, data1\n",
    "y_train, y_test = target, target1\n",
    "np.random.seed(sd)\n",
    "print(x_train[136])\n",
    "print(y_train[136])\n",
    "y1_train = dataset[[\"بازیگر\"]].values\n",
    "y1_test =  test_file[[\"بازیگر\"]].values\n",
    "y2_train = dataset[[\"بازیگری\"]].values\n",
    "y2_test =  test_file[[\"بازیگری\"]].values\n",
    "y3_train = dataset[[\"داستان\"]].values\n",
    "y3_test =  test_file[[\"داستان\"]].values\n",
    "\n",
    "y4_train = dataset[[\"سبک\"]].values\n",
    "y4_test =  test_file[[\"سبک\"]].values\n",
    "y5_train = dataset[[\"فیلم\"]].values\n",
    "y5_test =  test_file[[\"فیلم\"]].values\n",
    "y6_train = dataset[[\"فیلم_نامه\"]].values\n",
    "y6_test =  test_file[[\"فیلم_نامه\"]].values\n",
    "\n",
    "\n",
    "y7_train = dataset[[\"محتوا\"]].values\n",
    "y7_test =  test_file[[\"محتوا\"]].values\n",
    "\n",
    "y8_train = dataset[[\"موضوع\"]].values\n",
    "y8_test =  test_file[[\"موضوع\"]].values\n",
    "y9_train = dataset[[\"کارگردان\"]].values\n",
    "y9_test =  test_file[[\"کارگردان\"]].values\n",
    "\n",
    "#شمارش تعداد کلمات منحصر به فرد\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "all_words=' '.join(data)\n",
    "all_words=word_tokenize(all_words)\n",
    "dist=FreqDist(all_words)\n",
    "num_unique_word=len(dist)\n",
    "print ('number unique word:',num_unique_word)\n",
    "#شمارش بزرگترین طول متن\n",
    "r_len=[]\n",
    "for text in data:\n",
    "    word=word_tokenize(text)\n",
    "    l=len(word)\n",
    "    r_len.append(l)   \n",
    "MAX_REVIEW_LEN=np.max(r_len)\n",
    "print('max len:',MAX_REVIEW_LEN)\n",
    "np.random.seed(sd)\n",
    "num_words = num_unique_word\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "np.random.seed(sd)\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)\n",
    "print(x_train[100])\n",
    "print(x_train_tokens[100])\n",
    "max_tokens = MAX_REVIEW_LEN\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,padding='post')\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,padding='post')\n",
    "np.random.seed(sd)\n",
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
    "def tokens_to_string(tokens):\n",
    "    words = [inverse_map[token] for token in tokens if token!=0]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "print('train shape',x_train_pad.shape)\n",
    "print('test shape',x_test_pad.shape)\n",
    "np.random.seed(sd)\n",
    "\n",
    "#طراحی مدل\n",
    "embedding_size = 300\n",
    "input_1 = Input(shape=(max_tokens,))\n",
    "x=Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    name='embedding_layer',\n",
    "            embeddings_initializer=initializer)(input_1)\n",
    "x=Dropout(0.2,seed=sd)(x)\n",
    "x = Conv1D(256, 3, activation='relu')(x) ## conv layer 128 filters with a size of 2\n",
    "x = MaxPooling1D(1)(x)\n",
    "x = Attention()(x)\n",
    "x = Flatten()(x)\n",
    "output1 = Dense(3, activation='softmax',name='bazigar')(x)\n",
    "output2 = Dense(3, activation='softmax',name='bazigari')(x)\n",
    "output3 = Dense(3, activation='softmax',name='dastan')(x)\n",
    "output4 = Dense(3, activation='softmax',name='dialog')(x)\n",
    "output5 = Dense(3, activation='softmax')(x)\n",
    "output6 = Dense(3, activation='softmax')(x)\n",
    "output7 = Dense(3, activation='softmax')(x)\n",
    "output8 = Dense(3, activation='softmax')(x)\n",
    "output9 = Dense(3, activation='softmax')(x)\n",
    "model = Model(inputs=input_1, outputs=[output1, output2, output3, output4, output5, output6,output7,output8,output9\n",
    "                                      ])\n",
    "\n",
    "optimizer =Nadam(learning_rate=1e-3)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizer,)\n",
    "model.summary()\n",
    "np.random.seed(sd)\n",
    "history=model.fit(x_train_pad, \n",
    "                   y=[y1_train,y2_train,y3_train,y4_train,\n",
    "                                  y5_train,y6_train,y7_train,\n",
    "                                  y8_train,y9_train]\n",
    "                   , epochs=20, batch_size=256,shuffle=False,callbacks=[tensorboard])\n",
    "\n",
    "predicted = model.predict(x_test_pad)\n",
    "pred_class = np.argmax(predicted, axis=-1) \n",
    "from sklearn.metrics import hamming_loss, accuracy_score,precision_score\n",
    "p=pred_class.T\n",
    "hm=[]\n",
    "ac=[]\n",
    "x1=[]\n",
    "x2=[]\n",
    "for i in range(len(y_test)):\n",
    "  hm.append(hamming_loss(y_test[i], p[i]))\n",
    "print(np.mean(hm))\n",
    "print (jacard(y_test, p))\n",
    "print (subsetAccuracy(y_test, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z-a-G7Y7zh0l",
    "outputId": "596c8f50-e59e-4cea-81c2-8c944e0a8e63",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "for i in range(9):\n",
    "  print(category_list[i])\n",
    "  print(classification_report(y_test[:,i], p[:,i]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
